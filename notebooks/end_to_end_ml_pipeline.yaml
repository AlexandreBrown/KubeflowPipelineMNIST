apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.2, pipelines.kubeflow.org/pipeline_compilation_time: '2021-10-05T14:06:32.324573',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "1", "name": "number_of_epochs",
      "optional": true, "type": "Integer"}, {"default": "120", "name": "train_batch_size",
      "optional": true, "type": "Integer"}, {"default": "120", "name": "test_batch_size",
      "optional": true, "type": "Integer"}, {"default": "0.1", "name": "learning_rate",
      "optional": true, "type": "Float"}], "name": "Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.2}
spec:
  entrypoint: pipeline
  templates:
  - name: download-datasets
    container:
      args: [--train-dataset, /tmp/outputs/train_dataset/data, --test-dataset, /tmp/outputs/test_dataset/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_datasets(
            train_dataset_path,
            test_dataset_path
        ):
            import torchvision.datasets as dsets
            import os

            os.makedirs(train_dataset_path)
            dsets.MNIST(root=train_dataset_path, train=True, download=True)

            os.makedirs(test_dataset_path)
            dsets.MNIST(root=test_dataset_path, train=False, download=True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Download datasets', description='')
        _parser.add_argument("--train-dataset", dest="train_dataset_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset", dest="test_dataset_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_datasets(**_parsed_args)
      image: public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2
    outputs:
      artifacts:
      - {name: download-datasets-test_dataset, path: /tmp/outputs/test_dataset/data}
      - {name: download-datasets-train_dataset, path: /tmp/outputs/train_dataset/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.2
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-dataset", {"outputPath": "train_dataset"}, "--test-dataset",
          {"outputPath": "test_dataset"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef download_datasets(\n    train_dataset_path,\n    test_dataset_path\n):\n    import
          torchvision.datasets as dsets\n    import os\n\n    os.makedirs(train_dataset_path)\n    dsets.MNIST(root=train_dataset_path,
          train=True, download=True)\n\n    os.makedirs(test_dataset_path)\n    dsets.MNIST(root=test_dataset_path,
          train=False, download=True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Download
          datasets'', description='''')\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset\", dest=\"test_dataset_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = download_datasets(**_parsed_args)\n"],
          "image": "public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2"}}, "name":
          "Download datasets", "outputs": [{"name": "train_dataset", "type": "Dataset"},
          {"name": "test_dataset", "type": "Dataset"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: evaluate-resnet-model
    container:
      args: [--test-batch-size, '{{inputs.parameters.test_batch_size}}', --test-dataset,
        /tmp/inputs/test_dataset/data, --model, /tmp/inputs/model/data, --mlpipeline-metrics,
        /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef evaluate_resnet_model(\n    test_batch_size,\n    test_dataset_path,\n\
        \    model_path,\n    mlpipeline_metrics_path\n):\n    import torch\n    import\
        \ torch.nn as nn\n    import torchvision.datasets as dsets\n    import json\n\
        \    from kubeflow_pipeline_sample.resnet.resnet_50 import ResNet50\n    from\
        \ kubeflow_pipeline_sample.evaluation.evaluate_accuracy import evaluate_accuracy\n\
        \    from tqdm import tqdm\n    from torchvision.transforms import Compose\n\
        \    from torchvision.transforms import Normalize\n    from torchvision.transforms\
        \ import Resize\n    from torchvision.transforms import ToTensor\n\n    IMAGE_SIZE\
        \ = 64\n\n    preprocessing = Compose([\n        Resize((IMAGE_SIZE, IMAGE_SIZE)),\
        \ \n        ToTensor(),\n        Normalize(mean=(0.5), std=(0.5))\n    ])\n\
        \    test_dataset_clean = dsets.MNIST(root=test_dataset_path, train=False,\
        \ download=False, transform=preprocessing)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset_clean,\
        \ batch_size=test_batch_size)\n\n    model = ResNet50(in_channels=1, classes=10)\n\
        \    model.load_state_dict(torch.load(model_path))\n    model.eval()\n   \
        \ device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"\
        )\n\n    accuracy = evaluate_accuracy(model, test_loader, test_dataset_clean,\
        \ device)\n\n    metrics = {\n        'metrics': [\n            {\n      \
        \        'name': 'accuracy',\n              'numberValue':  accuracy,\n  \
        \            'format': \"PERCENTAGE\",\n            }\n        ]\n    }\n\n\
        \    with open(mlpipeline_metrics_path, 'w') as metrics_file:\n        json.dump(metrics,\
        \ metrics_file)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Evaluate\
        \ resnet model', description='')\n_parser.add_argument(\"--test-batch-size\"\
        , dest=\"test_batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-dataset\", dest=\"test_dataset_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\"\
        , dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-metrics\", dest=\"mlpipeline_metrics_path\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = evaluate_resnet_model(**_parsed_args)\n"
      image: public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2
    inputs:
      parameters:
      - {name: test_batch_size}
      artifacts:
      - {name: train-resnet-model-model, path: /tmp/inputs/model/data}
      - {name: download-datasets-test_dataset, path: /tmp/inputs/test_dataset/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.2
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--test-batch-size", {"inputValue": "test_batch_size"}, "--test-dataset",
          {"inputPath": "test_dataset"}, "--model", {"inputPath": "model"}, "--mlpipeline-metrics",
          {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef evaluate_resnet_model(\n    test_batch_size,\n    test_dataset_path,\n    model_path,\n    mlpipeline_metrics_path\n):\n    import
          torch\n    import torch.nn as nn\n    import torchvision.datasets as dsets\n    import
          json\n    from kubeflow_pipeline_sample.resnet.resnet_50 import ResNet50\n    from
          kubeflow_pipeline_sample.evaluation.evaluate_accuracy import evaluate_accuracy\n    from
          tqdm import tqdm\n    from torchvision.transforms import Compose\n    from
          torchvision.transforms import Normalize\n    from torchvision.transforms
          import Resize\n    from torchvision.transforms import ToTensor\n\n    IMAGE_SIZE
          = 64\n\n    preprocessing = Compose([\n        Resize((IMAGE_SIZE, IMAGE_SIZE)),
          \n        ToTensor(),\n        Normalize(mean=(0.5), std=(0.5))\n    ])\n    test_dataset_clean
          = dsets.MNIST(root=test_dataset_path, train=False, download=False, transform=preprocessing)\n    test_loader
          = torch.utils.data.DataLoader(dataset=test_dataset_clean, batch_size=test_batch_size)\n\n    model
          = ResNet50(in_channels=1, classes=10)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    device
          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    accuracy
          = evaluate_accuracy(model, test_loader, test_dataset_clean, device)\n\n    metrics
          = {\n        ''metrics'': [\n            {\n              ''name'': ''accuracy'',\n              ''numberValue'':  accuracy,\n              ''format'':
          \"PERCENTAGE\",\n            }\n        ]\n    }\n\n    with open(mlpipeline_metrics_path,
          ''w'') as metrics_file:\n        json.dump(metrics, metrics_file)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate resnet model'',
          description='''')\n_parser.add_argument(\"--test-batch-size\", dest=\"test_batch_size\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset\",
          dest=\"test_dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\",
          dest=\"mlpipeline_metrics_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate_resnet_model(**_parsed_args)\n"], "image": "public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2"}},
          "inputs": [{"name": "test_batch_size", "type": "Integer"}, {"name": "test_dataset",
          "type": "Dataset"}, {"name": "model", "type": "Model"}], "name": "Evaluate
          resnet model", "outputs": [{"name": "mlpipeline_metrics", "type": "Metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"test_batch_size":
          "{{inputs.parameters.test_batch_size}}"}'}
  - name: explore-datasets
    container:
      args: [--train-dataset, /tmp/inputs/train_dataset/data, --test-dataset, /tmp/inputs/test_dataset/data,
        --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def explore_datasets(
            train_dataset_path,
            test_dataset_path,
            mlpipeline_ui_metadata_path
        ):
            import torchvision.datasets as dsets
            import json

            train = dsets.MNIST(root=train_dataset_path, train=True, download=False)
            test = dsets.MNIST(root=test_dataset_path, train=False, download=False)

            metadata = {
            'outputs' : [{
              'type': 'table',
              'storage': 'inline',
              'format': 'csv',
              'header': ["Training samples", "Test samples"],
              'source': f"{len(train)}, {len(test)}"
            }]
            }

            with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
                json.dump(metadata, metadata_file)

        import argparse
        _parser = argparse.ArgumentParser(prog='Explore datasets', description='')
        _parser.add_argument("--train-dataset", dest="train_dataset_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset", dest="test_dataset_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = explore_datasets(**_parsed_args)
      image: public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2
    inputs:
      artifacts:
      - {name: download-datasets-test_dataset, path: /tmp/inputs/test_dataset/data}
      - {name: download-datasets-train_dataset, path: /tmp/inputs/train_dataset/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.2
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-dataset", {"inputPath": "train_dataset"}, "--test-dataset",
          {"inputPath": "test_dataset"}, "--mlpipeline-ui-metadata", {"outputPath":
          "mlpipeline_ui_metadata"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef explore_datasets(\n    train_dataset_path,\n    test_dataset_path,\n    mlpipeline_ui_metadata_path\n):\n    import
          torchvision.datasets as dsets\n    import json\n\n    train = dsets.MNIST(root=train_dataset_path,
          train=True, download=False)\n    test = dsets.MNIST(root=test_dataset_path,
          train=False, download=False)\n\n    metadata = {\n    ''outputs'' : [{\n      ''type'':
          ''table'',\n      ''storage'': ''inline'',\n      ''format'': ''csv'',\n      ''header'':
          [\"Training samples\", \"Test samples\"],\n      ''source'': f\"{len(train)},
          {len(test)}\"\n    }]\n    }\n\n    with open(mlpipeline_ui_metadata_path,
          ''w'') as metadata_file:\n        json.dump(metadata, metadata_file)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Explore datasets'', description='''')\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset\",
          dest=\"test_dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = explore_datasets(**_parsed_args)\n"], "image": "public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2"}},
          "inputs": [{"name": "train_dataset", "type": "Dataset"}, {"name": "test_dataset",
          "type": "Dataset"}], "name": "Explore datasets", "outputs": [{"name": "mlpipeline_ui_metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: pipeline
    inputs:
      parameters:
      - {name: learning_rate}
      - {name: number_of_epochs}
      - {name: test_batch_size}
      - {name: train_batch_size}
    dag:
      tasks:
      - {name: download-datasets, template: download-datasets}
      - name: evaluate-resnet-model
        template: evaluate-resnet-model
        dependencies: [download-datasets, train-resnet-model]
        arguments:
          parameters:
          - {name: test_batch_size, value: '{{inputs.parameters.test_batch_size}}'}
          artifacts:
          - {name: download-datasets-test_dataset, from: '{{tasks.download-datasets.outputs.artifacts.download-datasets-test_dataset}}'}
          - {name: train-resnet-model-model, from: '{{tasks.train-resnet-model.outputs.artifacts.train-resnet-model-model}}'}
      - name: explore-datasets
        template: explore-datasets
        dependencies: [download-datasets]
        arguments:
          artifacts:
          - {name: download-datasets-test_dataset, from: '{{tasks.download-datasets.outputs.artifacts.download-datasets-test_dataset}}'}
          - {name: download-datasets-train_dataset, from: '{{tasks.download-datasets.outputs.artifacts.download-datasets-train_dataset}}'}
      - name: train-resnet-model
        template: train-resnet-model
        dependencies: [download-datasets]
        arguments:
          parameters:
          - {name: learning_rate, value: '{{inputs.parameters.learning_rate}}'}
          - {name: number_of_epochs, value: '{{inputs.parameters.number_of_epochs}}'}
          - {name: train_batch_size, value: '{{inputs.parameters.train_batch_size}}'}
          artifacts:
          - {name: download-datasets-train_dataset, from: '{{tasks.download-datasets.outputs.artifacts.download-datasets-train_dataset}}'}
  - name: train-resnet-model
    container:
      args: [--number-of-epochs, '{{inputs.parameters.number_of_epochs}}', --train-batch-size,
        '{{inputs.parameters.train_batch_size}}', --learning-rate, '{{inputs.parameters.learning_rate}}',
        --train-dataset, /tmp/inputs/train_dataset/data, --model, /tmp/outputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_resnet_model(\n    number_of_epochs,\n    train_batch_size,\n\
        \    learning_rate,\n    train_dataset_path,\n    model_path   \n):\n    import\
        \ torch \n    import torch.nn as nn\n    import torchvision.datasets as dsets\n\
        \    from tqdm import tqdm\n    from torchvision.transforms import Compose\n\
        \    from torchvision.transforms import Normalize\n    from torchvision.transforms\
        \ import Resize\n    from torchvision.transforms import ToTensor\n    from\
        \ kubeflow_pipeline_sample.resnet.resnet_50 import ResNet50\n    from kubeflow_pipeline_sample.training.trainer\
        \ import train_model\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available()\
        \ else \"cpu\")\n    model = ResNet50(in_channels=1, classes=10).to(device)\n\
        \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(),\
        \ lr = learning_rate)\n\n    IMAGE_SIZE = 64\n\n    preprocessing = Compose([\n\
        \        Resize((IMAGE_SIZE, IMAGE_SIZE)), \n        ToTensor(),\n       \
        \ Normalize(mean=(0.5), std=(0.5))\n    ])\n    train_dataset_clean = dsets.MNIST(root=train_dataset_path,\
        \ train=True, download=False, transform=preprocessing)\n    train_loader =\
        \ torch.utils.data.DataLoader(dataset=train_dataset_clean, batch_size=train_batch_size)\n\
        \n    losses=train_model(\n        model=model,\n        train_loader=train_loader,\n\
        \        criterion=criterion,\n        optimizer=optimizer,\n        n_epochs=number_of_epochs,\n\
        \        device=device\n    )\n    torch.save(model.state_dict(), model_path)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Train resnet model',\
        \ description='')\n_parser.add_argument(\"--number-of-epochs\", dest=\"number_of_epochs\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --train-batch-size\", dest=\"train_batch_size\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--learning-rate\", dest=\"\
        learning_rate\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --train-dataset\", dest=\"train_dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = train_resnet_model(**_parsed_args)\n"
      image: public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2
    inputs:
      parameters:
      - {name: learning_rate}
      - {name: number_of_epochs}
      - {name: train_batch_size}
      artifacts:
      - {name: download-datasets-train_dataset, path: /tmp/inputs/train_dataset/data}
    outputs:
      artifacts:
      - {name: train-resnet-model-model, path: /tmp/outputs/model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.2
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--number-of-epochs", {"inputValue": "number_of_epochs"}, "--train-batch-size",
          {"inputValue": "train_batch_size"}, "--learning-rate", {"inputValue": "learning_rate"},
          "--train-dataset", {"inputPath": "train_dataset"}, "--model", {"outputPath":
          "model"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_resnet_model(\n    number_of_epochs,\n    train_batch_size,\n    learning_rate,\n    train_dataset_path,\n    model_path   \n):\n    import
          torch \n    import torch.nn as nn\n    import torchvision.datasets as dsets\n    from
          tqdm import tqdm\n    from torchvision.transforms import Compose\n    from
          torchvision.transforms import Normalize\n    from torchvision.transforms
          import Resize\n    from torchvision.transforms import ToTensor\n    from
          kubeflow_pipeline_sample.resnet.resnet_50 import ResNet50\n    from kubeflow_pipeline_sample.training.trainer
          import train_model\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available()
          else \"cpu\")\n    model = ResNet50(in_channels=1, classes=10).to(device)\n\n    criterion
          = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(),
          lr = learning_rate)\n\n    IMAGE_SIZE = 64\n\n    preprocessing = Compose([\n        Resize((IMAGE_SIZE,
          IMAGE_SIZE)), \n        ToTensor(),\n        Normalize(mean=(0.5), std=(0.5))\n    ])\n    train_dataset_clean
          = dsets.MNIST(root=train_dataset_path, train=True, download=False, transform=preprocessing)\n    train_loader
          = torch.utils.data.DataLoader(dataset=train_dataset_clean, batch_size=train_batch_size)\n\n    losses=train_model(\n        model=model,\n        train_loader=train_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        n_epochs=number_of_epochs,\n        device=device\n    )\n    torch.save(model.state_dict(),
          model_path)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          resnet model'', description='''')\n_parser.add_argument(\"--number-of-epochs\",
          dest=\"number_of_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-batch-size\",
          dest=\"train_batch_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--learning-rate\",
          dest=\"learning_rate\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_resnet_model(**_parsed_args)\n"], "image": "public.ecr.aws/h3o0w0k1/kubeflow-pipeline-mnist:v2"}},
          "inputs": [{"name": "number_of_epochs", "type": "Integer"}, {"name": "train_batch_size",
          "type": "Integer"}, {"name": "learning_rate", "type": "Float"}, {"name":
          "train_dataset", "type": "Dataset"}], "name": "Train resnet model", "outputs":
          [{"name": "model", "type": "Model"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"learning_rate": "{{inputs.parameters.learning_rate}}",
          "number_of_epochs": "{{inputs.parameters.number_of_epochs}}", "train_batch_size":
          "{{inputs.parameters.train_batch_size}}"}'}
  arguments:
    parameters:
    - {name: number_of_epochs, value: '1'}
    - {name: train_batch_size, value: '120'}
    - {name: test_batch_size, value: '120'}
    - {name: learning_rate, value: '0.1'}
  serviceAccountName: pipeline-runner
